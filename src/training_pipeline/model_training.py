import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, explained_variance_score, r2_score
from sklearn.model_selection import RandomizedSearchCV
import mlflow
import mlflow.sklearn
import numpy as np
import xgboost as xgb

def load_data(file_path):
    """
    Charge les donn√©es enrichies depuis un fichier CSV.
    """
    return pd.read_csv(file_path)

def evaluate_model(model, X_train, X_test, y_train, y_test):
    """
    Entra√Æne et √©value un mod√®le, retourne les m√©triques.
    """
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # Calcul des m√©triques
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    explained_var = explained_variance_score(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    return {
        "mse": mse,
        "rmse": rmse,
        "mae": mae,
        "explained_variance": explained_var,
        "r2": r2
    }

def optimize_xgboost(X_train, y_train):
    """
    Effectue une recherche d'hyperparam√®tres pour XGBoost.
    """
    param_dist = {
        "n_estimators": [50, 100, 200],
        "max_depth": [3, 5, 10],
        "learning_rate": [0.01, 0.1, 0.2],
        "subsample": [0.6, 0.8, 1.0],
        "colsample_bytree": [0.6, 0.8, 1.0]
    }

    xgb_model = xgb.XGBRegressor()
    random_search = RandomizedSearchCV(
        estimator=xgb_model,
        param_distributions=param_dist,
        n_iter=10,  # Nombre d'it√©rations pour la recherche
        scoring="r2",
        cv=3,
        verbose=1,
        n_jobs=-1,
        random_state=42
    )
    random_search.fit(X_train, y_train)

    print(f"Meilleurs hyperparam√®tres : {random_search.best_params_}")
    return random_search.best_estimator_

def train_and_evaluate_xgboost(data_path):
    """
    Entra√Æne et √©value le mod√®le XGBoost, puis sauvegarde le mod√®le et ses m√©triques dans MLflow.
    """
    # Charger les donn√©es
    print(f"Chargement des donn√©es depuis {data_path}...")
    data = load_data(data_path)

    # S√©parer les features (X) et la cible (y)
    X = data[['CO2', 'Humedad', 'PM2.5', 'Temperatura']]
    y = data['health_score']

    # Diviser les donn√©es en ensembles d'entra√Ænement et de test
    train_size = int(0.8 * len(data))
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]

    # Optimisation des hyperparam√®tres pour XGBoost
    print("Optimisation des hyperparam√®tres pour XGBoost...")
    best_xgb_model = optimize_xgboost(X_train, y_train)

    # √âvaluation du mod√®le optimis√©
    xgb_metrics = evaluate_model(best_xgb_model, X_train, X_test, y_train, y_test)

    print("\nüìä R√©sultats du mod√®le XGBoost optimis√© :")
    print(f"R¬≤ : {xgb_metrics['r2']:.2f}")
    print(f"MSE : {xgb_metrics['mse']:.2f}")
    print(f"RMSE : {xgb_metrics['rmse']:.2f}")
    print(f"MAE : {xgb_metrics['mae']:.2f}")
    print(f"Explained Variance : {xgb_metrics['explained_variance']:.2f}")

    # Sauvegarder le mod√®le et les m√©triques dans MLflow
    log_xgboost_to_mlflow(best_xgb_model, xgb_metrics)

def log_xgboost_to_mlflow(model, metrics):
    """
    Enregistre le mod√®le XGBoost et ses m√©triques dans MLflow.
    """
    mlflow.set_tracking_uri("../mlruns")  # Chemin vers le r√©pertoire MLflow
    mlflow.set_experiment("best_model")  # D√©finit ou cr√©e une exp√©rience appel√©e "best_model"

    with mlflow.start_run():
        # Enregistrer les m√©triques
        for metric_name, metric_value in metrics.items():
            mlflow.log_metric(metric_name, metric_value)

        # Enregistrer le mod√®le
        mlflow.sklearn.log_model(model, "best_model")

        print("‚úÖ Mod√®le XGBoost optimis√© enregistr√© avec succ√®s dans MLflow.")

if __name__ == "__main__":
    data_csv = "../../data/processed/final_enriched_data.csv"
    train_and_evaluate_xgboost(data_csv)