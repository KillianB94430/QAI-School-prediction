import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import mlflow
from mlflow.tracking import MlflowClient
import sys

# === 1Ô∏è‚É£ Charger les Donn√©es ===
real_data_path = "../../data/processed/final_enriched_data.csv"
pred_data_path = "../../data/processed/inference_predictions.csv"

df_real = pd.read_csv(real_data_path)
df_pred = pd.read_csv(pred_data_path)

# Convertir time_stamp en format datetime
df_real["time_stamp"] = pd.to_datetime(df_real["time_stamp"])
df_pred["time_stamp"] = pd.to_datetime(df_pred["time_stamp"])

# === 2Ô∏è‚É£ S√©lectionner une p√©riode sp√©cifique ===
def select_date_range(df, start_date, end_date):
    """Filtrer les donn√©es selon une plage de dates."""
    return df[(df["time_stamp"] >= start_date) & (df["time_stamp"] <= end_date)]

# R√©cup√©ration des arguments pass√©s par main.py
if len(sys.argv) >= 3:
    start_date = sys.argv[1]
    end_date = sys.argv[2]
else:
    raise ValueError("‚ö†Ô∏è Les dates de d√©but et de fin sont requises en argument.")


start_date = pd.to_datetime(start_date)
end_date = pd.to_datetime(end_date)

# Filtrer les datasets
df_real_filtered = select_date_range(df_real, start_date, end_date)
df_pred_filtered = select_date_range(df_pred, start_date, end_date)

# === 3Ô∏è‚É£ Fusion des Donn√©es sur 'time_stamp' et 'ID' ===
df_compare = df_real_filtered.merge(df_pred_filtered, on=["time_stamp", "ID"], how="inner")

# V√©rifier qu'on a bien des donn√©es apr√®s filtrage
if df_compare.empty:
    print("‚ùå Aucune donn√©e disponible pour cette p√©riode. V√©rifiez vos dates !")
    exit()

# === 4Ô∏è‚É£ Calcul des Erreurs ===
df_compare["error"] = abs(df_compare["health_score"] - df_compare["health_score_prediction"])

# M√©triques globales
mae = mean_absolute_error(df_compare["health_score"], df_compare["health_score_prediction"])
rmse = np.sqrt(mean_squared_error(df_compare["health_score"], df_compare["health_score_prediction"]))
r2 = r2_score(df_compare["health_score"], df_compare["health_score_prediction"])

print("\nüìä **√âvaluation du Mod√®le sur la p√©riode s√©lectionn√©e**")
print(f"‚û°Ô∏è MAE (Mean Absolute Error) : {mae:.2f}")
print(f"‚û°Ô∏è RMSE (Root Mean Squared Error) : {rmse:.2f}")
print(f"‚û°Ô∏è R¬≤ Score : {r2:.2f}")

# === 5Ô∏è‚É£ Syst√®me de Monitoring ===
ALERT_THRESHOLD = 0.05  # Seuil de 5% de d√©gradation tol√©r√©e
mlflow.set_tracking_uri("../mlruns")

def get_last_run_metrics(experiment_name="best_model"):
    """
    R√©cup√®re les m√©triques du dernier run dans une exp√©rience MLflow donn√©e.
    """
    client = MlflowClient()
    experiment = client.get_experiment_by_name(experiment_name)
    if experiment is None:
        raise ValueError(f"L'exp√©rience '{experiment_name}' n'existe pas.")
    
    # R√©cup√©rer la derni√®re ex√©cution de l'exp√©rience
    runs = client.search_runs(
        experiment_ids=[experiment.experiment_id],
        order_by=["start_time DESC"],
        max_results=1
    )
    
    if not runs:
        raise ValueError(f"Aucune ex√©cution trouv√©e pour l'exp√©rience '{experiment_name}'.")
    
    last_run = runs[0]
    # R√©cup√©rer toutes les m√©triques du dernier run
    return last_run.data.metrics

def monitor_model_performance(new_metrics):
    """
    Compare les nouvelles m√©triques avec celles du dernier run MLflow.
    """
    last_metrics = get_last_run_metrics()
    alert = False
    print("\nüìä Comparaison des performances du mod√®le :")
    for metric, last_value in last_metrics.items():
        new_value = new_metrics.get(metric, None)
        if new_value is not None:
            degradation = (last_value - new_value) / last_value if last_value != 0 else 0
            print(f"{metric}: Last = {last_value:.4f}, New = {new_value:.4f}, Degradation = {degradation:.2%}")
            if degradation > ALERT_THRESHOLD:
                alert = True
                print(f"‚ö†Ô∏è ALERTE: D√©gradation significative d√©tect√©e sur {metric}!")
    if not alert:
        print("‚úÖ Le mod√®le maintient ses performances. Pas d'alerte.")

# Ex√©cuter le monitoring
new_metrics = {"mse": mean_squared_error(df_compare["health_score"], df_compare["health_score_prediction"]),
               "rmse": rmse,
               "mae": mae,
               "r2": r2}
monitor_model_performance(new_metrics)

# === 6Ô∏è‚É£ Visualisation des R√©sultats ===
## üìç Comparaison Pr√©dictions vs Valeurs R√©elles
plt.figure(figsize=(10, 6))
plt.scatter(df_compare["health_score"], df_compare["health_score_prediction"], alpha=0.5)
plt.plot([df_compare["health_score"].min(), df_compare["health_score"].max()],
         [df_compare["health_score"].min(), df_compare["health_score"].max()],
         color="red", linestyle="--")
plt.xlabel("V√©rit√© Terrain (health_score)")
plt.ylabel("Pr√©dictions (health_score_prediction)")
plt.title(f"Comparaison des Pr√©dictions vs V√©rit√©s Terrain\n({start_date.date()} ‚Üí {end_date.date()})")
plt.show()